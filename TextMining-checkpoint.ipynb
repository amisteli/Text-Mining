{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>734</td>\n",
       "      <td>a31658.txt</td>\n",
       "      <td>if u dont mind she needs change she will be al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>766</td>\n",
       "      <td>ans1689.txt</td>\n",
       "      <td>Vaginal bleeding/spotting that occurs in betwe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>941</td>\n",
       "      <td>a61238.txt</td>\n",
       "      <td>I dont know. See a shrink.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>ans1239.txt</td>\n",
       "      <td>Thank you for your question. The use of antibi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1072</td>\n",
       "      <td>a61372.txt</td>\n",
       "      <td>It hurts like you wouldnt imagine! I had train...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>ans1117.txt</td>\n",
       "      <td>Thank you for your question. A severe peanut a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1833</td>\n",
       "      <td>ans962.txt</td>\n",
       "      <td>Pain in the stomach after meals, especially wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>ans1377.txt</td>\n",
       "      <td>The lower back pain has many causes, from main...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1614</td>\n",
       "      <td>ans765.txt</td>\n",
       "      <td>It is quite common to experience pain in the f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1027</td>\n",
       "      <td>a61327.txt</td>\n",
       "      <td>MAKE LOVE WITH SOME ONE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>ans1101.txt</td>\n",
       "      <td>Taking the contraceptive pill before your peri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>658</td>\n",
       "      <td>ans1591.txt</td>\n",
       "      <td>There may be many causes of lack of concentrat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>ans1355.txt</td>\n",
       "      <td>The heavy bleeding that you are experiencing c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>ans1055.txt</td>\n",
       "      <td>Since you have stopped breast feeding, it may ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>758</td>\n",
       "      <td>a33.txt</td>\n",
       "      <td>It has been reported that the mortality rate o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>931</td>\n",
       "      <td>ans1837.txt</td>\n",
       "      <td>Your symptoms are most probably due to Acute s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>ans1047.txt</td>\n",
       "      <td>Several studies demonstrated a small reduction...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1433</td>\n",
       "      <td>ans601.txt</td>\n",
       "      <td>If this is the first time and you put it back ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             file                                               text  class\n",
       "734    a31658.txt  if u dont mind she needs change she will be al...      0\n",
       "766   ans1689.txt  Vaginal bleeding/spotting that occurs in betwe...      1\n",
       "941    a61238.txt                        I dont know. See a shrink.       0\n",
       "267   ans1239.txt  Thank you for your question. The use of antibi...      1\n",
       "1072   a61372.txt  It hurts like you wouldnt imagine! I had train...      0\n",
       "132   ans1117.txt  Thank you for your question. A severe peanut a...      1\n",
       "1833   ans962.txt  Pain in the stomach after meals, especially wi...      1\n",
       "420   ans1377.txt  The lower back pain has many causes, from main...      1\n",
       "1614   ans765.txt  It is quite common to experience pain in the f...      1\n",
       "1027   a61327.txt                           MAKE LOVE WITH SOME ONE       0\n",
       "115   ans1101.txt  Taking the contraceptive pill before your peri...      1\n",
       "658   ans1591.txt  There may be many causes of lack of concentrat...      1\n",
       "396   ans1355.txt  The heavy bleeding that you are experiencing c...      1\n",
       "63    ans1055.txt  Since you have stopped breast feeding, it may ...      1\n",
       "758       a33.txt  It has been reported that the mortality rate o...      0\n",
       "931   ans1837.txt  Your symptoms are most probably due to Acute s...      1\n",
       "54    ans1047.txt  Several studies demonstrated a small reduction...      1\n",
       "1433   ans601.txt  If this is the first time and you put it back ...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data into a pandas dataframe\n",
    "\n",
    "def data2df (path, label):\n",
    "    file, text = [], []\n",
    "    for f in os.listdir(path):\n",
    "        file.append(f)\n",
    "        fhr = open(path+f, 'r', encoding='utf-8', errors='ignore') \n",
    "        t = fhr.read()\n",
    "        text.append(t)\n",
    "        fhr.close()\n",
    "    return(pd.DataFrame({'file': file, 'text': text, 'class':label}))\n",
    "\n",
    "dfneg = data2df('HealthProNonPro/NonPro/', 0) # NEG\n",
    "dfpos = data2df('HealthProNonPro/Pro/', 1) # POS\n",
    "\n",
    "df = pd.concat([dfpos, dfneg], axis=0)\n",
    "df.sample(frac=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the data\n",
    "X, y = df['text'], df['class']\n",
    "\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "Xtrain = Xtrain.copy()\n",
    "Xtest = Xtest.copy()\n",
    "ytrain = ytrain.copy()\n",
    "ytest = ytest.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(doc):\n",
    "\n",
    "    # use spacy to filter out noise\n",
    "    tokens = [token.lemma_.lower() \n",
    "                        for token in doc \n",
    "                               if (\n",
    "                                    len(token) >= 2 and # only preserve tokens that are greater than 2 characters long\n",
    "                                    token.pos_ in ['PROPN', 'NOUN', 'ADJ', 'VERB', 'ADV'] and # only preserve selected pos\n",
    "                                    #token.text in nlp.vocab and # check if token in vocab \n",
    "                                    token.is_alpha and # only preserve tokens that are fully alpha (not numeric or alpha-numeric)\n",
    "                                    #not token.is_digit and # get rid of tokens that are fully numeric\n",
    "                                    not token.is_punct and # get rid of tokens that are punctuations\n",
    "                                    not token.is_space and # get rid of tokens that are spaces\n",
    "                                    not token.is_stop and # get rid of tokens that are stop words\n",
    "                                    not token.is_currency # get rid of tokens that denote currencies\n",
    "                                )\n",
    "                   ]\n",
    "\n",
    "    # return cleaned-up text\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    common cause itchy palm contact dermatitis exp...\n",
       "1    take money support famlys need food shelter do...\n",
       "2                         sign married doctor approval\n",
       "3    speed amphetamine psychostimulant commonly abu...\n",
       "4                                          tantric sex\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=['parser', 'ner'])\n",
    "corpus = nlp.pipe(list(Xtrain))\n",
    "clean_corpus = [custom_tokenizer(doc) for doc in corpus]\n",
    "X = pd.Series(clean_corpus)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the preprocessing->model pipeline\n",
    "\n",
    "clf = Pipeline(steps=[('tfidf', TfidfVectorizer()),('nb', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup grid search\n",
    "\n",
    "param_grid = {\n",
    "    'nb__alpha': [0, 1], \n",
    "    'tfidf__sublinear_tf':[True,False] \n",
    "}\n",
    "gscv = GridSearchCV(clf, param_grid, cv=4, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aurélien\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\Aurélien\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\Aurélien\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\Aurélien\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\Aurélien\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\Aurélien\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\Aurélien\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "C:\\Users\\Aurélien\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Pipeline(memory=None,\n",
      "         steps=[('tfidf',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=1.0, max_features=None,\n",
      "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=True,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('nb',\n",
      "                 MultinomialNB(alpha=1, class_prior=None, fit_prior=True))],\n",
      "         verbose=False) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "gscv.fit(Xtrain, ytrain)\n",
    "\n",
    "print (\"-\"*100)\n",
    "print(gscv.best_estimator_, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9427012278308322\n",
      "[[317  41]\n",
      " [  1 374]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94       358\n",
      "           1       0.90      1.00      0.95       375\n",
      "\n",
      "    accuracy                           0.94       733\n",
      "   macro avg       0.95      0.94      0.94       733\n",
      "weighted avg       0.95      0.94      0.94       733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict and evaluate best_estimator_ on test data\n",
    "\n",
    "ypred = gscv.predict(Xtest)\n",
    "\n",
    "from sklearn import metrics\n",
    "print (metrics.accuracy_score(ytest, ypred))\n",
    "print (metrics.confusion_matrix(ytest, ypred))\n",
    "print (metrics.classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN, FP, FN, TP = metrics.confusion_matrix(y_true=ytest, y_pred=ypred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision0: 1.0\n",
      "Precision1: 0.9\n",
      "\n",
      "Recall0: 0.89\n",
      "Recall1: 1.0\n",
      "\n",
      "F1_Score: 0.94\n",
      "F1_Score: 0.95\n",
      "\n",
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "precision0=round(TP/(TP+FN),2)\n",
    "precision1=round(TP/(TP+FP),2)\n",
    "print(\"Precision0:\",precision0)\n",
    "print(\"Precision1:\",precision1)\n",
    "\n",
    "print()\n",
    "\n",
    "recall0=round(TN/(TN+FP),2)\n",
    "recall1=round(TP/(TP+FN),2)\n",
    "print(\"Recall0:\",recall0)\n",
    "print(\"Recall1:\",recall1)\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "F_1_0=round((2*precision0*recall0)/(precision0+recall0),2)\n",
    "F_1_1=round((2*precision1*recall1)/(precision1+recall1),2)\n",
    "print(\"F1_Score:\",F_1_0)\n",
    "print(\"F1_Score:\",F_1_1)\n",
    "\n",
    "print()\n",
    "\n",
    "Accuracy=round((TN+TP)/(TN+FP+FN+TP),2)\n",
    "print(\"Accuracy:\",Accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
